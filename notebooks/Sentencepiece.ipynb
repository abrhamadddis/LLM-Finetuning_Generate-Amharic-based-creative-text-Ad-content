{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '▁ኤምባሲ']\n",
      "[434, 111, 997, 1507]\n",
      "_በአዲስ_አበባ_የአሜሪካ_ኤምባሲ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: TIKVAH.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# train sentencepiece model from `TIKVAH.txt` and makes `m.model` and `m.vocab`\n",
    "# `m.vocab` is just a reference. not used in the segmentation.\n",
    "spm.SentencePieceTrainer.train('--input=yene-tube.txt --model_prefix=m --vocab_size=2000')\n",
    "\n",
    "# makes segmenter instance and loads the model file (m.model)\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "# encode: text => id\n",
    "print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\n",
    "print(sp.encode_as_ids('በአዲስ አበባ የአሜሪካ ኤምባሲ'))\n",
    "\n",
    "# decode: id => text\n",
    "print(sp.decode_pieces(['_በአዲስ', '_አበባ', '_የአሜሪካ', '_ኤ', 'ምባሲ']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "በአዲስ አበባ የአሜሪካ ኤምባሲ\n"
     ]
    }
   ],
   "source": [
    "print(sp.decode_ids([434, 111, 997, 1507]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "▁ቤቶች\n",
      "434\n",
      "0\n",
      "<unk> False\n",
      "<s> True\n",
      "</s> True\n"
     ]
    }
   ],
   "source": [
    "# returns vocab size\n",
    "print(sp.get_piece_size())\n",
    "\n",
    "# id <=> piece conversion\n",
    "print(sp.id_to_piece(460))\n",
    "print(sp.piece_to_id('▁በአዲስ'))\n",
    "\n",
    "# returns 0 for unknown tokens (we can change the id for UNK)\n",
    "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
    "\n",
    "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
    "# <s> and </s> are defined as 'control' symbol.\n",
    "for id in range(3):\n",
    "  print(sp.id_to_piece(id), sp.is_control(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m783.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.35.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.60.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow) (1.16.0)\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m366.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /home/abrham/.local/lib/python3.10/site-packages (from tensorflow) (4.9.0)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/abrham/.local/lib/python3.10/site-packages (from tensorflow) (69.0.2)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/abrham/.local/lib/python3.10/site-packages (from tensorflow) (4.25.1)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/abrham/.local/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/abrham/.local/lib/python3.10/site-packages (from tensorflow) (1.26.2)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.27.0-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.8/186.8 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 KB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.3.6)\n",
      "Collecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /home/abrham/.local/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/abrham/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/abrham/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.5)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.0)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, ml-dtypes, MarkupSafe, keras, h5py, grpcio, google-pasta, gast, astunparse, absl-py, werkzeug, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: wrapt\n",
      "    Found existing installation: wrapt 1.16.0\n",
      "    Uninstalling wrapt-1.16.0:\n",
      "      Successfully uninstalled wrapt-1.16.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.1\n",
      "    Uninstalling protobuf-4.25.1:\n",
      "      Successfully uninstalled protobuf-4.25.1\n",
      "Successfully installed MarkupSafe-2.1.4 absl-py-2.1.0 astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-auth-2.27.0 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.60.0 h5py-3.10.0 keras-2.15.0 libclang-16.0.6 ml-dtypes-0.2.0 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.1 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.15.1 tensorboard-data-server-0.7.2 tensorflow-2.15.0.post1 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.35.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 07:54:36.063805: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-01 07:54:36.789499: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-01 07:54:36.789847: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-01 07:54:36.903698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-01 07:54:37.160098: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-01 07:54:37.163326: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-01 07:54:44.201267: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '▁ኤምባሲ']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Assumes that m.model is stored in non-Posix file system.\n",
    "serialized_model_proto = tf.io.gfile.GFile('m.model', 'rb').read()\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load_from_serialized_proto(serialized_model_proto)\n",
    "\n",
    "print(sp.encode_as_pieces('በአዲስ አበባ የአሜሪካ ኤምባሲ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '<sep>', '▁ኤምባሲ', '<cls>']\n",
      "3\n",
      "4\n",
      "3= <sep>\n",
      "4= <cls>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: TIKVAH.txt\n",
      "  input_format: \n",
      "  model_prefix: m_user\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <sep>\n",
      "  user_defined_symbols: <cls>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m_user.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_user.vocab\n"
     ]
    }
   ],
   "source": [
    "# Example of user defined symbols\n",
    "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
    "\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load('m_user.model')\n",
    "\n",
    "# ids are reserved in both mode.\n",
    "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
    "# user defined symbols allow these symbol to apper in the text.\n",
    "print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\n",
    "print(sp_user.piece_to_id('<sep>'))  # 3\n",
    "print(sp_user.piece_to_id('<cls>'))  # 4\n",
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '<sep>', '▁ኤምባሲ', '<cls>']\n",
      "3\n",
      "4\n",
      "3= <sep>\n",
      "4= <cls>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: TIKVAH.txt\n",
      "  input_format: \n",
      "  model_prefix: m_user\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <sep>\n",
      "  user_defined_symbols: <cls>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m_user.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_user.vocab\n"
     ]
    }
   ],
   "source": [
    "# Example of user defined symbols\n",
    "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
    "\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load('m_user.model')\n",
    "\n",
    "# ids are reserved in both mode.\n",
    "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
    "# user defined symbols allow these symbol to apper in the text.\n",
    "print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\n",
    "print(sp_user.piece_to_id('<sep>'))  # 3\n",
    "print(sp_user.piece_to_id('<cls>'))  # 4\n",
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁በአዲስ', '▁አበባ', '▁የአሜሪካ', '<sep>', '▁ኤምባሲ', '<cls>']\n",
      "3\n",
      "4\n",
      "3= <sep>\n",
      "4= <cls>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: TIKVAH.txt\n",
      "  input_format: \n",
      "  model_prefix: m_user\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <sep>\n",
      "  user_defined_symbols: <cls>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <sep>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <cls>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m_user.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_user.vocab\n"
     ]
    }
   ],
   "source": [
    "# Example of user defined symbols\n",
    "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_user --user_defined_symbols=<sep>,<cls> --vocab_size=2000')\n",
    "\n",
    "sp_user = spm.SentencePieceProcessor()\n",
    "sp_user.load('m_user.model')\n",
    "\n",
    "# ids are reserved in both mode.\n",
    "# <unk>=0, <s>=1, </s>=2, <sep>=3, <cls>=4\n",
    "# user defined symbols allow these symbol to apper in the text.\n",
    "print(sp_user.encode_as_pieces('በአዲስ አበባ የአሜሪካ<sep> ኤምባሲ<cls>'))\n",
    "print(sp_user.piece_to_id('<sep>'))  # 3\n",
    "print(sp_user.piece_to_id('<cls>'))  # 4\n",
    "print('3=', sp_user.decode_ids([3]))  # decoded to <sep>\n",
    "print('4=', sp_user.decode_ids([4]))  # decoded to <cls>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<', 's', '>', '▁በአዲስ', '<', '/', 's', '>']\n",
      "['▁', '<s>', '▁በአዲስ', '</s>']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: TIKVAH.txt\n",
      "  input_format: \n",
      "  model_prefix: m_bos_as_user\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  user_defined_symbols: <s>\n",
      "  user_defined_symbols: </s>\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m_bos_as_user.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m_bos_as_user.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m_bos_as_user --user_defined_symbols=<s>,</s> --vocab_size=2000')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "print(sp.encode_as_pieces('<s> በአዲስ</s>'))   # <s>,</s> are segmented. (default behavior)\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m_bos_as_user.model')\n",
    "print(sp.encode_as_pieces('<s> በአዲስ</s>'))   # <s>,</s> are handled as one token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos= 1\n",
      "eos= 2\n",
      "unk= 0\n",
      "pad= -1\n",
      "[434, 111]\n",
      "[1, 434, 111, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=TIKVAH.txt --model_prefix=m --vocab_size=2000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: TIKVAH.txt\n",
      "  input_format: \n",
      "  model_prefix: m\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: TIKVAH.txt\n",
      "trainer_interface.cc(378) LOG(WARNING) Found too long line (5737 > 4192).\n",
      "trainer_interface.cc(380) LOG(WARNING) Too long lines are skipped in the training.\n",
      "trainer_interface.cc(381) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 16433 sentences\n",
      "trainer_interface.cc(414) LOG(INFO) Skipped 275 too long sentences.\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=4897676\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9503% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=298\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999503\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 16433 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2198202\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 158353 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 16433\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 135175\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 135175 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=75748 obj=13.0676 num_tokens=270036 num_tokens/piece=3.56493\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=66392 obj=11.5167 num_tokens=271825 num_tokens/piece=4.09424\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=49766 obj=11.5326 num_tokens=289026 num_tokens/piece=5.8077\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=49719 obj=11.4941 num_tokens=289114 num_tokens/piece=5.81496\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=37289 obj=11.6551 num_tokens=311892 num_tokens/piece=8.36418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=37285 obj=11.6114 num_tokens=311959 num_tokens/piece=8.36688\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=27963 obj=11.836 num_tokens=336675 num_tokens/piece=12.04\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=27963 obj=11.782 num_tokens=336709 num_tokens/piece=12.0412\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=20972 obj=12.0624 num_tokens=362050 num_tokens/piece=17.2635\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20972 obj=12.0004 num_tokens=362061 num_tokens/piece=17.264\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15729 obj=12.3357 num_tokens=387804 num_tokens/piece=24.6553\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15729 obj=12.264 num_tokens=387825 num_tokens/piece=24.6567\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11796 obj=12.6664 num_tokens=414411 num_tokens/piece=35.1315\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11796 obj=12.5802 num_tokens=414520 num_tokens/piece=35.1407\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8847 obj=13.027 num_tokens=440171 num_tokens/piece=49.7537\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8847 obj=12.935 num_tokens=440175 num_tokens/piece=49.7542\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=6635 obj=13.43 num_tokens=466090 num_tokens/piece=70.2472\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=6635 obj=13.3266 num_tokens=466338 num_tokens/piece=70.2846\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=4976 obj=13.8758 num_tokens=493170 num_tokens/piece=99.1097\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=4976 obj=13.7585 num_tokens=493810 num_tokens/piece=99.2383\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3732 obj=14.3751 num_tokens=520309 num_tokens/piece=139.418\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=3732 obj=14.2457 num_tokens=520312 num_tokens/piece=139.419\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2799 obj=14.9114 num_tokens=547282 num_tokens/piece=195.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2799 obj=14.775 num_tokens=547891 num_tokens/piece=195.745\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=2200 obj=15.373 num_tokens=570962 num_tokens/piece=259.528\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2200 obj=15.2486 num_tokens=570964 num_tokens/piece=259.529\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: m.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: m.vocab\n"
     ]
    }
   ],
   "source": [
    "spm.SentencePieceTrainer.train('--input=TIKVAH.txt --model_prefix=m --vocab_size=2000')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('m.model')\n",
    "\n",
    "print('bos=', sp.bos_id())\n",
    "print('eos=', sp.eos_id())\n",
    "print('unk=', sp.unk_id())\n",
    "print('pad=', sp.pad_id())  # disabled by default\n",
    "\n",
    "\n",
    "print(sp.encode_as_ids('በአዲስ አበባ'))\n",
    "\n",
    "# Prepend or append bos/eos ids.\n",
    "print([sp.bos_id()] + sp.encode_as_ids('በአዲስ አበባ') + [sp.eos_id()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
