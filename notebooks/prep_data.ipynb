{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define the directory where your .txt files are located\n",
    "input_directory = \"/home/abrham/Desktop/10x/week - 7/LLM-Finetuning_Generate-Amharic-based-creative-text-Ad-contents/notebooks\"\n",
    "\n",
    "# Define the directory where you want to save the .jsonl files\n",
    "output_directory = \"/home/abrham/Desktop/10x/week - 7/LLM-Finetuning_Generate-Amharic-based-creative-text-Ad-contents/notebooks\"\n",
    "\n",
    "# Ensure the output directory exists, if not create it\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Iterate through each .txt file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\"train_dataset.txt\"):\n",
    "        with open(os.path.join(input_directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "            # Read the contents of the text file\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                # Construct the JSON object\n",
    "                data = {\n",
    "                    \"input\": line.strip()\n",
    "                }\n",
    "\n",
    "                # Define the output file path\n",
    "                output_filename = os.path.splitext(filename)[0] + \".jsonl\"\n",
    "                output_path = os.path.join(output_directory, output_filename)\n",
    "\n",
    "                # Write the JSON object to the .jsonl file\n",
    "                with open(output_path, \"a\", encoding=\"utf-8\") as json_file:\n",
    "                    json.dump(data, json_file, ensure_ascii=False)\n",
    "                    json_file.write(\"\\n\")  # Add newline to separate entries in .jsonl file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 41907/41907 [00:21<00:00, 1957.16 examples/s]\n",
      "Map: 100%|██████████| 4001/4001 [00:01<00:00, 2555.44 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading the dataset\n",
    "train_dataset = load_dataset('json', data_files='train_dataset.jsonl', split='train')\n",
    "eval_dataset = load_dataset('json', data_files='eval_dataset.jsonl', split='train')\n",
    "\n",
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('mt.model') # Replace with the path to your trained SentencePiece model\n",
    "\n",
    "def formatting_func(example):\n",
    "    return example['input']\n",
    "\n",
    "def generate_and_tokenize_prompt(example):\n",
    "    formatted_text = formatting_func(example)\n",
    "    tokenized_ids = sp.EncodeAsIds(formatted_text)\n",
    "    return {\"tokenized_input\": tokenized_ids}\n",
    "\n",
    "# Apply the tokenization function to the datasets\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'tokenized_input'],\n",
       "    num_rows: 41907\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Input: [477, 298, 812, 440, 605, 5, 150, 493, 18, 3, 532, 449, 6, 41, 6, 15, 708, 217, 1040, 413, 3, 498, 55, 477, 307, 291, 298, 812, 440, 605, 5, 150, 493, 18, 3, 532, 449, 6, 41, 6, 15, 708, 217, 4, 1040, 413, 1830, 34, 32, 6, 41, 59, 5, 135, 20, 5, 135, 4, 269, 88, 383, 1013, 31, 63, 1040, 223, 239, 57, 11, 826, 1633, 117, 8, 924, 374, 687, 179, 694, 588, 568]\n",
      "Tokenized Input: [209, 223, 951, 1401, 23, 3, 1857, 3, 97, 18, 10, 129, 19, 266, 27, 1366, 3, 59, 11, 32, 42, 272, 81, 27, 30, 373, 483, 209, 223, 951, 1401, 8, 83, 28, 344, 3, 252, 105, 508, 26, 4, 6, 3, 21, 210, 252, 3, 1007, 252, 495, 11, 216, 61, 4, 4, 16, 37, 106, 282, 8, 719, 269, 88, 23, 3, 865, 10, 162, 1111, 5, 6, 114, 21, 6, 8, 243, 465, 38, 124, 1564, 14, 611, 223, 408, 57, 321, 87, 441, 388, 233, 972, 33, 159]\n",
      "Tokenized Input: [90, 3, 88, 29, 12, 13, 1175, 40, 728, 123, 861, 814, 553, 21, 310, 55, 5, 196, 303, 3, 981, 375, 108, 90, 3, 88, 29, 12, 13, 1175, 40, 4, 728, 123, 861, 1088, 3, 565, 29, 36, 1193, 3, 88, 29, 12, 13, 5, 376, 12, 861, 814, 56, 685, 310, 6, 117, 187, 8, 1035, 9, 8, 913, 5, 196, 303, 273, 352, 252, 105, 508, 45, 4, 59, 8, 7, 32, 11, 351, 3, 191, 6, 3, 349, 34, 9, 3, 68, 17, 15, 128, 20, 3, 53, 147, 29, 420]\n",
      "Tokenized Input: [466, 111, 89, 906, 16, 45, 44, 66, 16, 182, 182, 19, 1596, 10, 677, 53, 10, 24, 58, 1310, 562, 604, 143, 319, 3, 517, 36, 906, 16, 45, 44, 66, 16, 182, 182, 143, 319, 3, 127, 12, 74, 4, 16, 122, 98, 27, 52, 692, 1514, 96, 1374, 163, 77, 11, 123, 652, 22, 966, 90, 16, 49, 12, 16, 11, 41, 4, 577, 244, 580, 10, 962, 1350, 969, 650, 547, 192, 3, 1051, 1702]\n",
      "Tokenized Input: [5, 25, 622, 134, 95, 1205, 16, 97, 44, 15, 11, 83, 105, 491, 13, 1061, 288, 54, 296, 348, 30, 821, 36, 173, 634, 11, 91, 216, 32, 13, 30, 62, 199, 216, 688, 3, 1395, 298, 418, 3, 302, 6, 11, 33, 11, 32, 293, 622, 134, 95, 1205, 16, 97, 44, 15, 11, 83, 105, 491, 13, 1061, 288, 54, 296, 348, 550, 304, 207, 1696, 298, 57, 537, 639, 263, 3, 112, 5, 182, 54, 32, 11, 13, 185, 33, 876, 1061, 288, 54, 296, 226, 10, 8, 424, 82, 9, 930, 78, 4, 214, 3, 231, 41, 496, 6, 3, 46, 18, 333, 3, 266, 20, 177, 186, 190, 500, 12, 30, 53, 147, 1533, 36]\n"
     ]
    }
   ],
   "source": [
    "# Iterate over the first few examples of the tokenized validation dataset\n",
    "for example in tokenized_val_dataset.select(range(5)):\n",
    "    print(\"Tokenized Input:\", example[\"tokenized_input\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Tokens: 538249\n"
     ]
    }
   ],
   "source": [
    "total_tokens = sum(len(example[\"tokenized_input\"]) for example in tokenized_val_dataset)\n",
    "print(\"Total Number of Tokens:\", total_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
